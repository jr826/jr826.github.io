# About Me
I am current junior at Cornell University majoring in Electrical and Computer Engineering and pursuing minors in Computer Science and Game Design.  
My **coursework** features: Computer Systems Programming, Intelligent Physical Systems, Object-Oriented Programming and Data Structures, Digital Logic and Computer Organization, Embedded Systems (S'19), and Introduction to Microelectronics (S'19).  
Some **skills** of mine are: C, C++, Java, Python, Arduino, UNIX, Verilog, Assembly, Microcontrollers, MATLAB,  and Git.  
My **extracurriculars** include: Cornell Theme Park Entertainment Group (Member), Cornell Buds Men's Ultimate Frisbee Team (Member), Cornell Orientation Steering Committee (Orientation Leader), and the Big Red Pep Band (Member).  
## Experience
This past summer, I worked under Cornell Professor David Winkler as a member of the [TABER](http://eeb.cornell.edu/winkler/wordpress/?page_id=335) group. We used TI microcontrollers to design bird tags that would collect, store, and trasmit data to a basestation. My specific focus was on the memory management of the system. The programming of the microcontroller was done in C.
### Projects
**Maze Mapping Robot:** [This robot](https://mb2372.github.io/ece3400-team1) was a class project that I worked on with three other undergraduates. The robot was capable of following lines, detecting walls, avoiding other robots, mapping a maze, and transmitting the maze data to a nearby basestation. The robot was controlled by an Arduino and the base station was an Arduino as well. An FGPA was also used to detect shapes and colors of 'treasures' scattered throughout the maze. Verilog was used for the FPGA coding.  

**G-Love:** [This project](https://github.com/jr826/cornell_makeathon_2019) was created for the [CU Make-a-thon](http://www.cu-make.com). Following the prompt of creating art, our team created a LED filled glove for the use of sign language interpreters in environments such as concerts where it can be hard for people to see the interpreter due to distance or lighting. The glove lights different parts of the hands to be different colors, helping to make the exact signs being made clearer to viewers. The LEDs will also flash more rapidly as the glove is moved more quickly. This reflects the energy and tone of the language being signed to viewers. The G-Love uses a Kionix KX126 sensor with a built in accelerometer to determine the rate of movement of the glove. This data is sent to an Arduino that controls the LEDs on the glove.  

**Animatronic Phoenix:** As a member of the controls subteam of the [Cornell TPEG](https://cornelltpeg.weebly.com) I helped to design and implement the control mechanisms of this project. An Arduino was used to control three servos that each operated different parts of the phoenix. Each servo had to operate in a different manner to give us the desired movement of the phoenix.
